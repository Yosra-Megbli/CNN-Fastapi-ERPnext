#!/usr/bin/env python3
"""
Script d'entra√Ænement pour classification de documents
Target: 85%+ accuracy sur 4 classes
Dataset: Note (201), Invoice (247), Report (265), Drawing (128)
Total: 841 images
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

print("="*80)
print("üéØ ENTRA√éNEMENT DU MOD√àLE DE CLASSIFICATION DE DOCUMENTS")
print("="*80)

# ============================================================================
# CONFIGURATION
# ============================================================================
CONFIG = {
    'image_size': (224, 224),
    'batch_size': 16,
    'epochs': 50,
    'learning_rate': 0.0001,
    'validation_split': 0.2,
    'test_split': 0.15,
    'target_accuracy': 0.85,
    'patience': 10,  # Early stopping
    'classes': ['Drawing', 'Invoice', 'Report', 'Note'],  # Ordre alphab√©tique
    'data_augmentation': True
}

print(f"\nüìã Configuration:")
for key, value in CONFIG.items():
    print(f"   ‚Ä¢ {key}: {value}")

# ============================================================================
# CHEMINS DES DONN√âES
# ============================================================================
# Structure attendue:
# dataset/
#   ‚îú‚îÄ‚îÄ Drawing/
#   ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg
#   ‚îÇ   ‚îî‚îÄ‚îÄ img2.jpg
#   ‚îú‚îÄ‚îÄ Invoice/
#   ‚îú‚îÄ‚îÄ Report/
#   ‚îî‚îÄ‚îÄ Note/

DATA_DIR = "../dataset"  # Ajustez selon votre structure

if not os.path.exists(DATA_DIR):
    print(f"\n‚ùå ERREUR: Dossier {DATA_DIR} introuvable!")
    print(f"\nüìÅ Structure attendue:")
    print(f"   {DATA_DIR}/")
    print(f"   ‚îú‚îÄ‚îÄ Drawing/")
    print(f"   ‚îú‚îÄ‚îÄ Invoice/")
    print(f"   ‚îú‚îÄ‚îÄ Report/")
    print(f"   ‚îî‚îÄ‚îÄ Note/")
    exit(1)

# ============================================================================
# CHARGEMENT DES DONN√âES
# ============================================================================
print(f"\nüì• Chargement du dataset depuis: {DATA_DIR}")

# Utiliser image_dataset_from_directory (meilleure pratique)
try:
    # Dataset complet
    full_dataset = keras.utils.image_dataset_from_directory(
        DATA_DIR,
        labels='inferred',
        label_mode='categorical',
        image_size=CONFIG['image_size'],
        batch_size=CONFIG['batch_size'],
        shuffle=True,
        seed=42
    )
    
    class_names = full_dataset.class_names
    print(f"\n‚úÖ Classes d√©tect√©es: {class_names}")
    
    # Compter les images par classe
    total_images = sum([len(os.listdir(os.path.join(DATA_DIR, cls))) 
                       for cls in class_names if os.path.isdir(os.path.join(DATA_DIR, cls))])
    print(f"‚úÖ Total d'images: {total_images}")
    
    for cls in class_names:
        cls_path = os.path.join(DATA_DIR, cls)
        if os.path.isdir(cls_path):
            count = len(os.listdir(cls_path))
            percentage = (count / total_images) * 100
            print(f"   ‚Ä¢ {cls}: {count} images ({percentage:.1f}%)")

except Exception as e:
    print(f"‚ùå Erreur lors du chargement: {e}")
    exit(1)

# ============================================================================
# SPLIT TRAIN/VALIDATION/TEST - CORRECTION ICI
# ============================================================================
# Obtenir le nombre total de batches
total_batches = tf.data.experimental.cardinality(full_dataset).numpy()

# Calculer les tailles en nombre de batches
test_size = int(total_batches * CONFIG['test_split'])
val_size = int(total_batches * CONFIG['validation_split'])
train_size = total_batches - val_size - test_size

# S'assurer qu'on a au moins 1 batch pour chaque split
test_size = max(1, test_size)
val_size = max(1, val_size)
train_size = max(1, train_size)

# Cr√©er les datasets
train_dataset = full_dataset.take(train_size)
remaining = full_dataset.skip(train_size)
val_dataset = remaining.take(val_size)
test_dataset = remaining.skip(val_size)

print(f"\nüìä Split des donn√©es:")
print(f"   ‚Ä¢ Total batches: {total_batches}")
print(f"   ‚Ä¢ Train: {train_size} batches (~{train_size * CONFIG['batch_size']} images)")
print(f"   ‚Ä¢ Validation: {val_size} batches (~{val_size * CONFIG['batch_size']} images)")
print(f"   ‚Ä¢ Test: {test_size} batches (~{test_size * CONFIG['batch_size']} images)")

# ============================================================================
# DATA AUGMENTATION
# ============================================================================
if CONFIG['data_augmentation']:
    print(f"\nüîÑ Configuration de l'augmentation des donn√©es...")
    
    data_augmentation = keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1),
        layers.RandomContrast(0.1),
        layers.RandomBrightness(0.1),
    ])
    print("‚úÖ Augmentation activ√©e")

# Optimisation des performances
AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)
val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)

# ============================================================================
# CR√âATION DU MOD√àLE
# ============================================================================
print(f"\nüèóÔ∏è  Construction du mod√®le...")

# Base: EfficientNetB0 (meilleur rapport performance/taille)
base_model = keras.applications.EfficientNetB0(
    input_shape=(*CONFIG['image_size'], 3),
    include_top=False,
    weights='imagenet'
)

# Geler les couches de base initialement
base_model.trainable = False

# Cr√©er le mod√®le complet
inputs = keras.Input(shape=(*CONFIG['image_size'], 3))

# Augmentation (si activ√©e)
if CONFIG['data_augmentation']:
    x = data_augmentation(inputs)
else:
    x = inputs

# Preprocessing sp√©cifique √† EfficientNet
x = keras.applications.efficientnet.preprocess_input(x)

# Base model
x = base_model(x, training=False)

# Classification head
x = layers.GlobalAveragePooling2D()(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.3)(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.3)(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(len(class_names), activation='softmax')(x)

model = keras.Model(inputs, outputs)

print(f"‚úÖ Mod√®le cr√©√©")
print(f"   ‚Ä¢ Architecture: EfficientNetB0 + Custom Head")
print(f"   ‚Ä¢ Param√®tres totaux: {model.count_params():,}")
print(f"   ‚Ä¢ Param√®tres entra√Ænables: {sum([tf.size(v).numpy() for v in model.trainable_variables]):,}")

# ============================================================================
# COMPILATION - PHASE 1 (Base gel√©e)
# ============================================================================
print(f"\n‚öôÔ∏è  Compilation du mod√®le (Phase 1: Base gel√©e)...")

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate'] * 10),
    loss='categorical_crossentropy',
    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')]
)

print("‚úÖ Mod√®le compil√©")

# ============================================================================
# CALLBACKS
# ============================================================================
print(f"\nüìû Configuration des callbacks...")

callbacks = [
    # Early stopping
    keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=CONFIG['patience'],
        restore_best_weights=True,
        verbose=1
    ),
    
    # R√©duction du learning rate
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    ),
    
    # Sauvegarde du meilleur mod√®le (format Keras natif)
    keras.callbacks.ModelCheckpoint(
        '../models/best_model_checkpoint.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    ),
    
    # TensorBoard (d√©sactiv√© histogram_freq pour √©viter erreurs de pickle)
    keras.callbacks.TensorBoard(
        log_dir='../logs',
        histogram_freq=0,  # 0 = d√©sactiv√© (√©vite erreurs deepcopy)
        write_graph=True,
        update_freq='epoch'
    )
]

print("‚úÖ Callbacks configur√©s")

# ============================================================================
# ENTRA√éNEMENT - PHASE 1
# ============================================================================
print("\n" + "="*80)
print("üöÄ PHASE 1: ENTRA√éNEMENT AVEC BASE GEL√âE")
print("="*80)

history_phase1 = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=20,  # Phase 1: moins d'epochs
    callbacks=callbacks,
    verbose=1
)

# √âvaluation apr√®s Phase 1
print(f"\nüìä √âvaluation apr√®s Phase 1...")
phase1_results = model.evaluate(test_dataset, verbose=0)
print(f"   ‚Ä¢ Test Loss: {phase1_results[0]:.4f}")
print(f"   ‚Ä¢ Test Accuracy: {phase1_results[1]:.4f} ({phase1_results[1]*100:.2f}%)")
print(f"   ‚Ä¢ Top-2 Accuracy: {phase1_results[2]:.4f} ({phase1_results[2]*100:.2f}%)")

# ============================================================================
# FINE-TUNING - PHASE 2 (D√©geler les derni√®res couches)
# ============================================================================
if phase1_results[1] < CONFIG['target_accuracy']:
    print("\n" + "="*80)
    print("üî• PHASE 2: FINE-TUNING (D√©gel des derni√®res couches)")
    print("="*80)
    
    # D√©geler les derni√®res couches de la base
    base_model.trainable = True
    
    # Geler les premi√®res couches, d√©geler les derni√®res
    fine_tune_at = len(base_model.layers) - 30  # D√©geler les 30 derni√®res couches
    
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
    
    print(f"‚úÖ D√©gel√© les {len(base_model.layers) - fine_tune_at} derni√®res couches")
    print(f"   ‚Ä¢ Param√®tres entra√Ænables: {sum([tf.size(v).numpy() for v in model.trainable_variables]):,}")
    
    # Recompiler avec un learning rate plus faible
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate'] / 10),
        loss='categorical_crossentropy',
        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')]
    )
    
    # Entra√Ænement Phase 2
    history_phase2 = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=CONFIG['epochs'],
        initial_epoch=history_phase1.epoch[-1],
        callbacks=callbacks,
        verbose=1
    )
    
    # Combiner les historiques
    history = {
        'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],
        'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],
        'loss': history_phase1.history['loss'] + history_phase2.history['loss'],
        'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss']
    }
else:
    history = history_phase1.history

# ============================================================================
# √âVALUATION FINALE
# ============================================================================
print("\n" + "="*80)
print("üìä √âVALUATION FINALE")
print("="*80)

final_results = model.evaluate(test_dataset, verbose=0)
test_loss = final_results[0]
test_accuracy = final_results[1]
top2_accuracy = final_results[2]

print(f"\nüéØ R√©sultats sur le test set:")
print(f"   ‚Ä¢ Loss: {test_loss:.4f}")
print(f"   ‚Ä¢ Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"   ‚Ä¢ Top-2 Accuracy: {top2_accuracy:.4f} ({top2_accuracy*100:.2f}%)")

if test_accuracy >= CONFIG['target_accuracy']:
    print(f"\n‚úÖ OBJECTIF ATTEINT! ({test_accuracy*100:.2f}% >= {CONFIG['target_accuracy']*100}%)")
else:
    print(f"\n‚ö†Ô∏è  Objectif non atteint ({test_accuracy*100:.2f}% < {CONFIG['target_accuracy']*100}%)")
    print(f"   Suggestions:")
    print(f"   ‚Ä¢ Augmenter les epochs")
    print(f"   ‚Ä¢ Ajouter plus de donn√©es")
    print(f"   ‚Ä¢ Essayer EfficientNetB3 (plus gros)")

# ============================================================================
# SAUVEGARDE DU MOD√àLE
# ============================================================================
print(f"\nüíæ Sauvegarde du mod√®le final...")

os.makedirs("../models", exist_ok=True)

try:
    # Format natif Keras (recommand√©, fonctionne toujours)
    model.save("../models/final_model_complete.keras")
    keras_size = os.path.getsize("../models/final_model_complete.keras") / (1024 * 1024)
    print(f"‚úÖ Sauvegard√©: ../models/final_model_complete.keras ({keras_size:.1f} MB)")
except Exception as e:
    print(f"‚ö†Ô∏è  Erreur sauvegarde .keras: {e}")

try:
    # Format SavedModel (production, TensorFlow Serving)
    model.save("../models/final_model_savedmodel", save_format='tf')
    print(f"‚úÖ Sauvegard√©: ../models/final_model_savedmodel/")
except Exception as e:
    print(f"‚ö†Ô∏è  Erreur sauvegarde SavedModel: {e}")

# Format .h5 : Cr√©er un mod√®le sans augmentation pour compatibilit√©
print(f"\nüîß Cr√©ation d'un mod√®le .h5 sans augmentation (pour compatibilit√©)...")
try:
    # Recr√©er le mod√®le sans data augmentation
    inputs_clean = keras.Input(shape=(*CONFIG['image_size'], 3))
    x_clean = keras.applications.efficientnet.preprocess_input(inputs_clean)
    x_clean = base_model(x_clean, training=False)
    x_clean = layers.GlobalAveragePooling2D()(x_clean)
    x_clean = layers.BatchNormalization()(x_clean)
    x_clean = layers.Dropout(0.3)(x_clean)
    x_clean = layers.Dense(256, activation='relu')(x_clean)
    x_clean = layers.BatchNormalization()(x_clean)
    x_clean = layers.Dropout(0.3)(x_clean)
    x_clean = layers.Dense(128, activation='relu')(x_clean)
    x_clean = layers.Dropout(0.2)(x_clean)
    outputs_clean = layers.Dense(len(class_names), activation='softmax')(x_clean)
    
    model_h5 = keras.Model(inputs_clean, outputs_clean)
    
    # Copier les poids du mod√®le entra√Æn√© (skip les layers d'augmentation)
    for layer_original, layer_h5 in zip(model.layers[1:], model_h5.layers):
        if layer_original.name == layer_h5.name:
            try:
                layer_h5.set_weights(layer_original.get_weights())
            except:
                pass  # Skip si incompatible
    
    model_h5.save("../models/final_model_complete.h5")
    h5_size = os.path.getsize("../models/final_model_complete.h5") / (1024 * 1024)
    print(f"‚úÖ Sauvegard√©: ../models/final_model_complete.h5 ({h5_size:.1f} MB)")
    file_size = h5_size
except Exception as e:
    print(f"‚ö†Ô∏è  Impossible de sauvegarder en .h5: {e}")
    print(f"   ‚Üí Utilisez le format .keras √† la place")
    file_size = keras_size if 'keras_size' in locals() else 0

# ============================================================================
# VISUALISATION (optionnel)
# ============================================================================
print(f"\nüìà G√©n√©ration des graphiques...")

plt.figure(figsize=(12, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history['accuracy'], label='Train Accuracy')
plt.plot(history['val_accuracy'], label='Val Accuracy')
plt.axhline(y=CONFIG['target_accuracy'], color='r', linestyle='--', label='Target (85%)')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss
plt.subplot(1, 2, 2)
plt.plot(history['loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('../models/training_history.png', dpi=300)
print(f"‚úÖ Graphiques sauvegard√©s: ../models/training_history.png")

# ============================================================================
# R√âSUM√â FINAL
# ============================================================================
print("\n" + "="*80)
print("üéâ ENTRA√éNEMENT TERMIN√â")
print("="*80)
print(f"\nüìä R√©sum√©:")
print(f"   ‚Ä¢ Dataset: {total_images} images, 4 classes")
print(f"   ‚Ä¢ Architecture: EfficientNetB0 + Transfer Learning")
print(f"   ‚Ä¢ Accuracy finale: {test_accuracy*100:.2f}%")
print(f"   ‚Ä¢ Top-2 Accuracy: {top2_accuracy*100:.2f}%")
print(f"   ‚Ä¢ Taille du mod√®le: {file_size:.1f} MB")

print(f"\nüöÄ Prochaines √©tapes:")
print(f"   1. Testez le mod√®le: python main.py")
print(f"   2. Le mod√®le devrait se charger rapidement")
print(f"   3. Mode R√âEL avec fusion CNN + OCR activ√©")

print(f"\nüí° Utilisation:")
print(f"   curl http://localhost:8000/api/v1/status")
print(f"   ‚Üí 'model_loaded': true, 'mode': 'real'")

print("="*80)